{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10539823,"sourceType":"datasetVersion","datasetId":6521450}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np # linear algebra\nimport cv2\nfrom pathlib import Path","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Goal:** Create a Machine Learning Model that is able to classify whether a video is AI generated or not\n\n\nMethod: Using CNN and Temporal Modeling as my starting point for this classification task","metadata":{}},{"cell_type":"code","source":"def extract_frames(video_path, num_frames=30, resize=(224, 224)):\n    \"\"\"\n    Extract evenly spaced frames from a video\n    \n    Args:\n        video_path: Path to video file\n        num_frames: Number of frames to extract\n        resize: Target size for frames (height, width)\n    \n    Returns:\n        numpy array of shape (num_frames, height, width, 3)\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Calculate frame indices to extract (evenly spaced)\n    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n    \n    frames = []\n    for idx in frame_indices:\n        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n        ret, frame = cap.read()\n        \n        if ret:\n            # Convert BGR to RGB\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            # Resize\n            frame = cv2.resize(frame, resize)\n            # Normalize pixel values to [0, 1]\n            frame = frame.astype(np.float32) / 255.0\n            frames.append(frame)\n    \n    cap.release()\n    \n    return np.array(frames)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T22:03:16.667443Z","iopub.execute_input":"2026-02-03T22:03:16.667812Z","iopub.status.idle":"2026-02-03T22:03:16.675657Z","shell.execute_reply.started":"2026-02-03T22:03:16.667781Z","shell.execute_reply":"2026-02-03T22:03:16.674690Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\ndef process_video_dataset(ai_video_dir, real_video_dir, num_frames=30):\n    \"\"\"\n    Process all videos and create labeled dataset\n    \"\"\"\n    X = []  # Features (frames)\n    y = []  # Labels (0=real, 1=AI)\n    \n    # Process AI-generated videos\n    print(\"Processing AI videos...\")\n    ai_videos = list(Path(ai_video_dir).glob('*.mp4'))\n    for video_path in tqdm(ai_videos):\n        try:\n            frames = extract_frames(str(video_path), num_frames)\n            X.append(frames)\n            y.append(1)  # AI label\n        except Exception as e:\n            print(f\"Error processing {video_path}: {e}\")\n    \n    # Process real videos\n    print(\"Processing real videos...\")\n    real_videos = list(Path(real_video_dir).glob('*.mp4'))\n    for video_path in tqdm(real_videos):\n        try:\n            frames = extract_frames(str(video_path), num_frames)\n            X.append(frames)\n            y.append(0)  # Real label\n        except Exception as e:\n            print(f\"Error processing {video_path}: {e}\")\n    \n    X = np.array(X)\n    y = np.array(y)\n    \n    return X, y\n\n# Usage\nX, y = process_video_dataset('/kaggle/input/realai-video-dataset/ai', \n                              '/kaggle/input/realai-video-dataset/real',\n                              num_frames=30)\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Labels shape: {y.shape}\")\n\n# Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T22:04:59.685814Z","iopub.execute_input":"2026-02-03T22:04:59.686586Z","iopub.status.idle":"2026-02-03T22:33:40.416535Z","shell.execute_reply.started":"2026-02-03T22:04:59.686550Z","shell.execute_reply":"2026-02-03T22:33:40.414572Z"}},"outputs":[{"name":"stdout","text":"Processing AI videos...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [04:31<00:00,  8.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"Processing real videos...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [24:07<00:00, 43.85s/it]  \n","output_type":"stream"},{"name":"stdout","text":"Dataset shape: (66, 30, 224, 224, 3)\nLabels shape: (66,)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def build_lightweight_model(input_shape):\n    \"\"\"\n    Smaller model that uses less memory\n    Good for Kaggle's GPU limits\n    \"\"\"\n    model = Sequential([\n        # Smaller 3D convolutions\n        Conv3D(8, (3, 3, 3), padding='same', activation='relu', input_shape=input_shape),\n        MaxPooling3D((2, 2, 2)),\n        BatchNormalization(),\n        \n        Conv3D(16, (3, 3, 3), padding='same', activation='relu'),\n        MaxPooling3D((2, 2, 2)),\n        BatchNormalization(),\n        \n        Conv3D(32, (3, 3, 3), padding='same', activation='relu'),\n        MaxPooling3D((2, 2, 2)),\n        BatchNormalization(),\n        \n        # Global average pooling instead of flatten (reduces parameters)\n        tf.keras.layers.GlobalAveragePooling3D(),\n        \n        Dense(128, activation='relu'),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')\n    ])\n    \n    return model\n\nmodel = build_lightweight_model(input_shape=X_train.shape[1:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T22:40:59.007843Z","iopub.execute_input":"2026-02-03T22:40:59.008206Z","iopub.status.idle":"2026-02-03T22:40:59.089568Z","shell.execute_reply.started":"2026-02-03T22:40:59.008177Z","shell.execute_reply":"2026-02-03T22:40:59.088694Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.Precision(name='precision'),\n        tf.keras.metrics.Recall(name='recall'),\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T22:41:14.487426Z","iopub.execute_input":"2026-02-03T22:41:14.487796Z","iopub.status.idle":"2026-02-03T22:41:14.506067Z","shell.execute_reply.started":"2026-02-03T22:41:14.487734Z","shell.execute_reply":"2026-02-03T22:41:14.505170Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def augment_video(video_frames):\n    \"\"\"\n    Apply random augmentations to video frames\n    \"\"\"\n    # Random horizontal flip\n    if np.random.random() > 0.5:\n        video_frames = video_frames[:, :, ::-1, :]\n    \n    # Random brightness adjustment\n    brightness_factor = np.random.uniform(0.8, 1.2)\n    video_frames = np.clip(video_frames * brightness_factor, 0, 1)\n    \n    # Random rotation (small angle)\n    angle = np.random.uniform(-5, 5)\n    # Note: You'd need to implement rotation for each frame\n    \n    return video_frames\n\n# Create augmented training data\ndef create_augmented_dataset(X, y, augmentation_factor=2):\n    \"\"\"\n    Create additional training samples through augmentation\n    \"\"\"\n    X_augmented = [X]\n    y_augmented = [y]\n    \n    for _ in range(augmentation_factor - 1):\n        X_aug = np.array([augment_video(video) for video in X])\n        X_augmented.append(X_aug)\n        y_augmented.append(y)\n    \n    return np.vstack(X_augmented), np.hstack(y_augmented)\n\n# Augment training data\nX_train_aug, y_train_aug = create_augmented_dataset(X_train, y_train, augmentation_factor=2)\nprint(f\"Augmented training data: {X_train_aug.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T22:41:27.402566Z","iopub.execute_input":"2026-02-03T22:41:27.402930Z","iopub.status.idle":"2026-02-03T22:41:30.073063Z","shell.execute_reply.started":"2026-02-03T22:41:27.402900Z","shell.execute_reply":"2026-02-03T22:41:30.071830Z"}},"outputs":[{"name":"stdout","text":"Augmented training data: (104, 30, 224, 224, 3)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from tensorflow.keras.callbacks import (\n    EarlyStopping, ModelCheckpoint, \n    ReduceLROnPlateau, TensorBoard\n)\nimport datetime\n\n# Setup callbacks\nlog_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ncallbacks = [\n    EarlyStopping(\n        monitor='val_loss',\n        patience=15,\n        restore_best_weights=True,\n        verbose=1\n    ),\n    \n    ModelCheckpoint(\n        'best_model.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    ),\n    \n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-7,\n        verbose=1\n    ),\n    \n    TensorBoard(log_dir=log_dir, histogram_freq=1)\n]\n\n# Train the model\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    epochs=50,\n    batch_size=4,  # Adjust based on your GPU memory\n    callbacks=callbacks,\n    verbose=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nimport seaborn as sns\n\n# Plot training history\ndef plot_training_history(history):\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Accuracy\n    axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n    axes[0].plot(history.history['val_accuracy'], label='Val Accuracy')\n    axes[0].set_title('Model Accuracy')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Accuracy')\n    axes[0].legend()\n    axes[0].grid(True)\n    \n    # Loss\n    axes[1].plot(history.history['loss'], label='Train Loss')\n    axes[1].plot(history.history['val_loss'], label='Val Loss')\n    axes[1].set_title('Model Loss')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Loss')\n    axes[1].legend()\n    axes[1].grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_training_history(history)\n\n# Predictions\ny_pred_proba = model.predict(X_test)\ny_pred = (y_pred_proba > 0.5).astype(int)\n\n# Classification Report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Real', 'AI']))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Real', 'AI'], \n            yticklabels=['Real', 'AI'])\nplt.title('Confusion Matrix')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the final model\nmodel.save('ai_video_detector_final.keras')\n\n# Load and use for predictions\nfrom tensorflow.keras.models import load_model\n\nloaded_model = load_model('ai_video_detector_final.keras')\n\ndef predict_video(video_path):\n    \"\"\"Predict if a video is AI-generated\"\"\"\n    frames = extract_frames(video_path, num_frames=30)\n    frames = np.expand_dims(frames, axis=0)  # Add batch dimension\n    \n    prediction = loaded_model.predict(frames)[0][0]\n    \n    if prediction > 0.5:\n        return f\"AI-Generated (confidence: {prediction*100:.2f}%)\"\n    else:\n        return f\"Real Video (confidence: {(1-prediction)*100:.2f}%)\"\n\n# Test on a new video\nresult = predict_video('/path/to/test_video.mp4')\nprint(result)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}